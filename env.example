# Cost Katana Backend Environment Variables

# ===================================
# Core Application Settings
# ===================================
NODE_ENV=development
PORT=8000
FRONTEND_URL=http://localhost:3000

# ===================================
# Database Configuration
# ===================================
MONGODB_URI=mongodb://localhost:27017/cost-katana

# ===================================
# Authentication & Security
# ===================================
JWT_SECRET=your-super-secret-jwt-key-change-this
JWT_EXPIRES_IN=7d
JWT_REFRESH_SECRET=your-refresh-secret-key-change-this
JWT_REFRESH_EXPIRES_IN=30d
ENCRYPTION_KEY=your-encryption-key-change-this

# ===================================
# AWS Configuration
# ===================================
AWS_ACCESS_KEY_ID=your-aws-access-key
AWS_SECRET_ACCESS_KEY=your-aws-secret-key
AWS_REGION=us-east-1
AWS_BEDROCK_REGION=us-east-1

# AWS Bedrock Settings
AWS_BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0
AWS_BEDROCK_MAX_TOKENS=4096
AWS_BEDROCK_TEMPERATURE=0.7

# AWS Bedrock Template-Specific Settings
AWS_BEDROCK_TEMPLATE_MODEL_ID=amazon.nova-micro-v1:0

# AWS S3 Settings
AWS_S3_BUCKET=cost-katana-uploads
AWS_S3_REGION=us-east-1

# ===================================
# Logging Configuration
# ===================================
# Enable/disable request logging middleware
ENABLE_REQUEST_LOGGING=true

# Log level: OFF, ERROR, WARN, INFO, DEBUG, VERBOSE
LOG_LEVEL=INFO

# ===================================
# Sentry Error Tracking & Performance Monitoring
# ===================================
# Sentry DSN for error tracking (optional)
SENTRY_DSN=your-sentry-dsn-here

# Sentry environment (development, staging, production)
SENTRY_ENVIRONMENT=development

# Sentry release version (auto-populated from package.json)
SENTRY_RELEASE=

# Error sampling rate (0.0 to 1.0, 1.0 = 100% of errors)
SENTRY_SAMPLE_RATE=1.0

# Performance tracing sample rate (0.0 to 1.0, lower for production)
SENTRY_TRACES_SAMPLE_RATE=0.1

# Profiling sample rate (0.0 to 1.0, lower for production)
SENTRY_PROFILES_SAMPLE_RATE=0.1

# Enable Sentry debug mode (logs Sentry internal operations)
SENTRY_DEBUG=false

# Server name for identifying instances
SENTRY_SERVER_NAME=cost-katana-backend

# Enable/disable performance monitoring
SENTRY_ENABLE_PERFORMANCE_MONITORING=true

# Enable/disable profiling
SENTRY_ENABLE_PROFILING=true

# Enable/disable error filtering
SENTRY_ENABLE_ERROR_FILTERING=true

# ===================================
# OpenTelemetry Configuration
# ===================================
# Service identification
OTEL_SERVICE_NAME=cost-katana-api

# OTLP Endpoints - Mode A: Direct to vendor
# Set to empty to disable OTLP export (recommended for development)
OTLP_HTTP_TRACES_URL=
OTLP_HTTP_METRICS_URL=

# Alternative: Use localhost collector if you have one running
# OTLP_HTTP_TRACES_URL=http://localhost:4318/v1/traces
# OTLP_HTTP_METRICS_URL=http://localhost:4318/v1/metrics

# Authentication headers for vendor (e.g., {"Authorization":"Bearer YOUR_TOKEN"})
# Must be valid JSON format
OTEL_EXPORTER_OTLP_HEADERS=

# TLS Certificate (optional, base64 encoded certificate content for secure connections)
OTEL_EXPORTER_OTLP_CERTIFICATE=

# Security settings for OTLP exporter
OTEL_EXPORTER_OTLP_INSECURE=true

# Privacy Settings
CK_CAPTURE_MODEL_TEXT=false

# Regional telemetry routing (auto, us, eu, ap)
CK_TELEMETRY_REGION=auto

# Optional: Enable/disable telemetry completely
TELEMETRY_ENABLED=true

# Optional: Collector watchdog for auto-restart (production)
ENABLE_COLLECTOR_WATCHDOG=false
COLLECTOR_WATCHDOG_INTERVAL=60000

# ===================================
# Telemetry Cleanup & Optimization
# ===================================
# Auto-delete telemetry data older than X days
TELEMETRY_TTL_DAYS=7

# Retention policies (in days)
TELEMETRY_ERROR_RETENTION_DAYS=30              # Keep errors longer
TELEMETRY_SUCCESS_RETENTION_DAYS=7             # Keep successful requests
TELEMETRY_VECTOR_RETENTION_DAYS=3              # Keep vector embeddings

# Sampling rate for successful requests (0.0 to 1.0)
# 0.1 = 10% of successful requests are stored, 0.5 = 50%, 1.0 = 100%
TELEMETRY_SAMPLE_RATE=0.1

# Enable/disable vector embeddings (consumes significant space)
ENABLE_TELEMETRY_VECTORIZATION=false

# ===================================
# Email Configuration
# ===================================
EMAIL_HOST=smtp.gmail.com
EMAIL_PORT=587
EMAIL_SECURE=false
EMAIL_USER=your-email@gmail.com
EMAIL_PASS=your-app-specific-password
EMAIL_FROM=Cost Katana <noreply@costkatana.com>

# ===================================
# Redis Configuration
# ===================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# ===================================
# External Services
# ===================================
# OpenAI
OPENAI_API_KEY=your-openai-api-key
OPENAI_ORG_ID=your-openai-org-id

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-api-key

# Google AI
GOOGLE_AI_API_KEY=your-google-ai-api-key

# Mixpanel Analytics
MIXPANEL_TOKEN=your-mixpanel-token
MIXPANEL_API_SECRET=your-mixpanel-api-secret

# LangSmith
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your-langsmith-api-key
LANGCHAIN_PROJECT=cost-katana

# ===================================
# Rate Limiting
# ===================================
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX_REQUESTS=100

# ===================================
# Logging
# ===================================
LOG_LEVEL=info
LOG_FILE_PATH=./logs

# ===================================
# CORS Configuration
# ===================================
CORS_ORIGIN=*

# ===================================
# API Keys for Testing
# ===================================
TEST_API_KEY=test-api-key-for-development

# ===================================
# Feature Flags
# ===================================
ENABLE_CACHING=true
ENABLE_RATE_LIMITING=true
ENABLE_TELEMETRY=true
ENABLE_SECURITY_MONITORING=true

# ===================================
# Performance Tuning
# ===================================
MAX_POOL_SIZE=10
CONNECTION_TIMEOUT=60000
REQUEST_TIMEOUT=30000

# ===================================
# Monitoring Endpoints (Optional)
# ===================================
# For Mode C: Individual Docker containers
TEMPO_ENDPOINT=http://localhost:3200
PROMETHEUS_ENDPOINT=http://localhost:9090
GRAFANA_ENDPOINT=http://localhost:3000

# ===================================
# Database Backup Configuration
# ===================================
# Enable automatic database backups
ENABLE_DB_BACKUP=true

# Backup frequency in hours (default: 12)
BACKUP_INTERVAL_HOURS=12

# Local backup directory
BACKUP_LOCAL_PATH=./backups

# Backup retention days (local and S3)
BACKUP_RETENTION_DAYS=30

# S3 backup configuration
BACKUP_S3_BUCKET=cost-katana-backups
BACKUP_S3_REGION=us-east-1
BACKUP_S3_PREFIX=database-backups

# MongoDB dump options
MONGODB_DUMP_OPTIONS=--gzip --archive

# Backup compression (gzip, bzip2, or none)
BACKUP_COMPRESSION=gzip

# ===================================
# RAG Configuration
# ===================================
# Enable/disable RAG ingestion system
ENABLE_RAG_INGESTION=true

# Embedding model for vector generation
RAG_EMBEDDING_MODEL=amazon.titan-embed-text-v2:0

# Chunking configuration
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200

# Sync interval in hours (how often to sync conversations/telemetry)
RAG_SYNC_INTERVAL_HOURS=6

# MongoDB Vector Search index name
MONGODB_VECTOR_INDEX_NAME=document_vector_index

# Ingestion Settings
INGEST_ON_STARTUP=false
INGEST_CONVERSATIONS=true
INGEST_TELEMETRY=true
MAX_DOCUMENT_SIZE_MB=10

# ===================================
# External Telemetry Polling
# ===================================
# Enable polling user's telemetry endpoints
ENABLE_TELEMETRY_POLLING=true

# How often to poll user endpoints (in minutes)
TELEMETRY_SYNC_INTERVAL_MINUTES=5

# Maximum number of traces to fetch per endpoint per poll
MAX_TRACES_PER_POLL=100

# ===================================
# GitHub Integration
# ===================================
# GitHub App credentials for auto-integration feature
GITHUB_APP_ID=
GITHUB_APP_PRIVATE_KEY=
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
GITHUB_WEBHOOK_SECRET=
GITHUB_CALLBACK_URL=

# ===================================
# Linear Integration
# ===================================
# Linear OAuth credentials for integration feature
LINEAR_CLIENT_ID=your-linear-client-id
LINEAR_CLIENT_SECRET=your-linear-client-secret
LINEAR_CALLBACK_URL=http://localhost:8000/api/integrations/linear/callback
BACKEND_URL=http://localhost:8000

# ===================================
# JIRA Integration
# ===================================
# JIRA OAuth credentials for integration feature
JIRA_CLIENT_ID=your-jira-client-id
JIRA_CLIENT_SECRET=your-jira-client-secret
JIRA_CALLBACK_URL=http://localhost:8000/api/integrations/jira/callback

# ===================================
# Development Settings
# ===================================
DEBUG=false
VERBOSE_LOGGING=false

# ============================================
# ðŸ§  Cortex Meta-Language Configuration
# ============================================
# Cortex is a semantic optimization layer that reduces LLM costs by 40-50%

# Core Configuration
CORTEX_ENABLED=true                                      # Enable/disable Cortex globally
CORTEX_MODE=optional                                     # mandatory | optional | disabled
CORTEX_DEFAULT_ENABLED=false                             # Default state when not specified

# Model Configuration (AWS Bedrock) - Optimized for maximum capability
# IMPORTANT: Claude Opus requires Provisioned Throughput ($8,640+/month minimum)
# To enable Opus: Go to AWS Console â†’ Bedrock â†’ Model access â†’ Request Provisioned Throughput
# System will automatically fall back to best available models if Opus is not accessible
CORTEX_ENCODER_MODEL=amazon.nova-pro-v1:0                        # Nova Pro for encoding ($0.80/1M tokens)
CORTEX_DECODER_MODEL=amazon.nova-pro-v1:0                        # Nova Pro for decoding ($0.80/1M tokens)
CORTEX_CORE_MODEL=anthropic.claude-opus-4-1-20250805-v1:0        # Claude Opus 4.1 for max reasoning (if available)
CORTEX_DEFAULT_MODEL=amazon.nova-pro-v1:0                        # Nova Pro fallback ($0.80/1M - best value)

# Gateway Model Selection (Users can override via headers/query params)
# Headers: X-Cortex-Core-Model, X-Cortex-Encoder-Model, X-Cortex-Decoder-Model
# Query: cortex-core-model=..., cortex-encoder-model=..., cortex-decoder-model=...
# Presets: X-Cortex-Preset or cortex-preset= (ultra-fast|balanced|high-quality|maximum-power|cost-optimized)
#
# Available Models (Pricing per 1M tokens):
# - amazon.nova-micro-v1:0         ($0.35 input, $1.40 output) - Ultra fast, basic tasks
# - amazon.nova-lite-v1:0          ($0.60 input, $2.40 output) - Fast, good quality
# - amazon.nova-pro-v1:0           ($0.80 input, $3.20 output) - Best value, strong reasoning
# - anthropic.claude-3-5-haiku     ($1.00 input, $5.00 output) - Fast Claude
# - anthropic.claude-3-sonnet      ($3.00 input, $15.00 output) - Balanced Claude
# - anthropic.claude-3-5-sonnet    ($3.00 input, $15.00 output) - Latest Sonnet
# - anthropic.claude-3-opus        ($15.00 input, $75.00 output) - Most powerful (requires special access)

# Optimization Features
CORTEX_TOKEN_REDUCTION=true                              # Enable token reduction optimization
CORTEX_USE_TOON=true                                     # Use TOON format instead of JSON (30-60% token reduction)
CORTEX_SEMANTIC_CACHING=true                             # Enable semantic caching
CORTEX_MODEL_ROUTING=true                                # Enable intelligent model routing
CORTEX_BINARY_SERIALIZATION=false                        # Enable binary serialization
CORTEX_NEURAL_COMPRESSION=false                          # Enable neural compression (uses embeddings)
CORTEX_FRAGMENT_CACHING=true                             # Enable fragment-level caching
CORTEX_PREDICTIVE_PREFETCHING=false                      # Enable predictive query prefetching

# High-Value Pending Features
CORTEX_UNIVERSAL_DATA_CONVERSION=true                    # Convert SQL/JSON/API data to Cortex format
CORTEX_SELF_CRITIQUE_LOOP=true                           # LLM critiques its own output before finalizing
CORTEX_RESPONSE_QUALITY_PREDICTION=true                  # Predict response quality before generation
CORTEX_DISTRIBUTED_EXECUTION=true                        # Execute independent tasks in parallel
CORTEX_DYNAMIC_PRIMITIVE_LEARNING=true                   # Learn new primitives from user interactions
CORTEX_ENHANCED_SCHEMA_VALIDATION=false                  # Full GraphQL-like schema enforcement

# Security & Encryption
CORTEX_ENCRYPTION_KEY=cortex-change-this-in-production   # AES-256 encryption key for binary format
CORTEX_ENABLE_ENCRYPTION=false                           # Enable binary format encryption
CORTEX_SECURITY_MODE=standard                            # standard | strict | maximum

# Gateway Configuration
CORTEX_HEADER_NAME=x-cortex-enabled                      # Header to enable Cortex
CORTEX_QUERY_PARAM=cortex                                # Query param to enable Cortex
CORTEX_COOKIE_NAME=cortex_enabled                        # Cookie name for persistent preference
CORTEX_ALLOW_OVERRIDE=true                               # Allow per-request override

# Cache Configuration
CORTEX_CACHE_PROVIDER=hybrid                             # redis | memory | hybrid
CORTEX_CACHE_TTL=3600                                    # Cache TTL in seconds
CORTEX_CACHE_MAX_SIZE=1000                               # Max cache entries
CORTEX_CACHE_EVICTION=lru                                # lru | lfu | fifo

# Plugin Configuration
CORTEX_PLUGINS=                                          # Comma-separated list of plugins
CORTEX_PLUGINS_AUTOLOAD=false                            # Auto-load plugins on startup
CORTEX_PLUGINS_DIR=                                      # Plugin directory path

# Monitoring Configuration
CORTEX_METRICS_ENABLED=true                              # Enable metrics collection
CORTEX_LOG_LEVEL=info                                    # debug | info | warn | error
CORTEX_TRACE_ENABLED=false                               # Enable execution tracing
CORTEX_SAMPLE_RATE=1.0                                   # Sampling rate for metrics (0.0-1.0)

# Performance & Quality Configuration
CORTEX_QUALITY_THRESHOLD=0.85                           # Minimum quality score for responses (0.0-1.0)
CORTEX_PARALLEL_TASK_LIMIT=10                            # Max tasks to execute in parallel
CORTEX_CRITIQUE_ITERATIONS=2                             # Max self-critique iterations
CORTEX_PRIMITIVE_LEARNING_RATE=0.1                       # Rate of learning new primitives (0.0-1.0)
CORTEX_DATA_CONVERSION_TIMEOUT=5000                      # Timeout for data format conversion (ms)

# Limits Configuration
CORTEX_MAX_EXPRESSION_DEPTH=10                           # Max nesting depth for expressions
CORTEX_MAX_ROLE_COUNT=50                                 # Max roles per expression
CORTEX_MAX_REFERENCE_DEPTH=5                             # Max reference chain depth
CORTEX_MAX_CACHE_SIZE=10000                              # Max total cache size
CORTEX_MAX_PRIMITIVE_COUNT=100000                        # Max primitives in vocabulary
CORTEX_MAX_PARALLEL_TASKS=20                             # Max concurrent task execution

# Data Source Configuration (for Universal Data Conversion)
CORTEX_SQL_CONNECTION_TIMEOUT=10000                      # SQL connection timeout (ms)
CORTEX_API_REQUEST_TIMEOUT=15000                         # External API request timeout (ms)
CORTEX_DATA_CACHE_TTL=1800                               # Cache converted data for 30 minutes
